{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LunarLander in module gym.envs.box2d.lunar_lander object:\n",
      "\n",
      "class LunarLander(gym.core.Env, gym.utils.ezpickle.EzPickle)\n",
      " |  The main OpenAI Gym class. It encapsulates an environment with\n",
      " |  arbitrary behind-the-scenes dynamics. An environment can be\n",
      " |  partially or fully observed.\n",
      " |  \n",
      " |  The main API methods that users of this class need to know are:\n",
      " |  \n",
      " |      step\n",
      " |      reset\n",
      " |      render\n",
      " |      close\n",
      " |      seed\n",
      " |  \n",
      " |  And set the following attributes:\n",
      " |  \n",
      " |      action_space: The Space object corresponding to valid actions\n",
      " |      observation_space: The Space object corresponding to valid observations\n",
      " |      reward_range: A tuple corresponding to the min and max possible rewards\n",
      " |  \n",
      " |  Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
      " |  \n",
      " |  The methods are accessed publicly as \"step\", \"reset\", etc.. The\n",
      " |  non-underscored versions are wrapper methods to which we may add\n",
      " |  functionality over time.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LunarLander\n",
      " |      gym.core.Env\n",
      " |      gym.utils.ezpickle.EzPickle\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the state of the environment and returns an initial observation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  continuous = False\n",
      " |  \n",
      " |  metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.utils.ezpickle.EzPickle:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(8,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Sequential):\n",
    "    def __init__(self, n_nodes=32, n_hidden=2):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.add(Dense(n_nodes, activation='relu', input_shape=env.observation_space.shape), )\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden):\n",
    "            self.add(Dense(n_nodes, activation='relu'), )\n",
    "        # output layer\n",
    "        self.add(Dense(env.action_space.n, activation='linear'),)\n",
    "        # compile\n",
    "        self.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    def clone_from(self, another):\n",
    "        self.set_weights(another.get_weights())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self._memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self._memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        samples = random.sample(self._memory, min(len(self._memory), batch_size))\n",
    "        batch = np.array(samples, dtype=object).transpose()\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        states, next_states = np.stack(states), np.stack(next_states)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, policy):\n",
    "        self._env = env\n",
    "        self._memory = Memory(100_000)\n",
    "        self._policy = policy\n",
    "        self._target = DQN().clone_from(self._policy)\n",
    "    \n",
    "    @property\n",
    "    def policy(self): return self._policy\n",
    "    \n",
    "    def _choose_action(self, state, *, epilson=0.5):\n",
    "        if np.random.random()>epilson:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(agent._policy(tf.constant([state])))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def play(self, *, n_steps=1000, render=False):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        for i_steps in range(1, n_steps+1):\n",
    "            action = self._choose_action(state, epilson=1)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if render: env.render()\n",
    "            rewards += reward\n",
    "            state = next_state\n",
    "            if done: break\n",
    "        if render: \n",
    "            print(f'Steps taken: {i_steps}, rewards earned: {rewards}')\n",
    "            env.close()\n",
    "        else:\n",
    "            return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def _train(self, *, batch_size=1024, gamma=0.99):\n",
    "        states, actions, rewards, next_states, dones = self._memory.get_batch(batch_size)\n",
    "        next_states_predicts = self._target(next_states).numpy()\n",
    "        q_targets = self._policy(states).numpy()\n",
    "        for i,row in enumerate(q_targets):\n",
    "            row[actions[i]] = rewards[i] if dones[i] else rewards[i] + gamma*np.max(next_states_predicts[i])\n",
    "        X, y = tf.constant(states), tf.constant(q_targets)\n",
    "        self._policy.fit(X, y, epochs=10, batch_size=len(X), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, *, n_eps=1000, n_steps=1000):\n",
    "        scores = deque(maxlen=50)\n",
    "        for i_eps in range(1, n_eps+1):            \n",
    "            state = env.reset()\n",
    "            done = False            \n",
    "            for _ in range(n_steps):\n",
    "                action = self._choose_action(state, epilson=i_eps/n_eps)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                self._memory.remember(state, action, reward, next_state, done)\n",
    "                state = next_state \n",
    "                if done: break\n",
    "            self._train()\n",
    "            if i_eps%10==0:\n",
    "                scores.append(self.play())\n",
    "            if i_eps%50==0:\n",
    "                self._target.clone_from(self._policy)\n",
    "            if i_eps%2==0:\n",
    "                print('#', end='')\n",
    "            if i_eps%100==0:                \n",
    "                mean_score = sum(scores)/len(scores)\n",
    "                print(f' | Episode {i_eps:>4d} | rewards: {mean_score:.1f}')\n",
    "                if i_eps>1000 and mean_score>=195:\n",
    "                    print(f'\\nMean score of {mean_score:.1f} has reached the target.')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: -296.3\n",
      "################################################## | Episode  200 | rewards: -327.2\n",
      "################################################## | Episode  300 | rewards: -271.7\n",
      "################################################## | Episode  400 | rewards: -247.1\n",
      "################################################## | Episode  500 | rewards: -234.2\n",
      "################################################## | Episode  600 | rewards: -194.1\n",
      "################################################## | Episode  700 | rewards: -151.7\n",
      "################################################## | Episode  800 | rewards: -130.5\n",
      "################################################## | Episode  900 | rewards: -103.8\n",
      "################################################## | Episode 1000 | rewards: -82.6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: -222.5\n",
      "################################################## | Episode  200 | rewards: -188.3\n",
      "################################################## | Episode  300 | rewards: -140.0\n",
      "################################################## | Episode  400 | rewards: -121.7\n",
      "################################################## | Episode  500 | rewards: -96.2\n",
      "################################################## | Episode  600 | rewards: -74.1\n",
      "################################################## | Episode  700 | rewards: -47.3\n",
      "################################################## | Episode  800 | rewards: -38.4\n",
      "################################################## | Episode  900 | rewards: 1.2\n",
      "################################################## | Episode 1000 | rewards: 29.2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 77.9\n",
      "################################################## | Episode  200 | rewards: 59.1\n",
      "################################################## | Episode  300 | rewards: 77.2\n",
      "################################################## | Episode  400 | rewards: 89.6\n",
      "################################################## | Episode  500 | rewards: 84.0\n",
      "################################################## | Episode  600 | rewards: 92.8\n",
      "################################################## | Episode  700 | rewards: 118.0\n",
      "################################################## | Episode  800 | rewards: 137.0\n",
      "################################################## | Episode  900 | rewards: 138.2\n",
      "################################################## | Episode 1000 | rewards: 155.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 20.7\n",
      "################################################## | Episode  200 | rewards: 81.9\n",
      "################################################## | Episode  300 | rewards: 68.6\n",
      "################################################## | Episode  400 | rewards: 54.9\n",
      "################################################## | Episode  500 | rewards: 51.5\n",
      "################################################## | Episode  600 | rewards: 58.2\n",
      "################################################## | Episode  700 | rewards: 20.2\n",
      "################################################## | Episode  800 | rewards: 22.5\n",
      "################################################## | Episode  900 | rewards: 37.8\n",
      "################################################## | Episode 1000 | rewards: 72.6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 213.9\n",
      "################################################## | Episode  200 | rewards: 193.5\n",
      "################################################## | Episode  300 | rewards: 184.4\n",
      "################################################## | Episode  400 | rewards: 184.0\n",
      "################################################## | Episode  500 | rewards: 171.7\n",
      "################################################## | Episode  600 | rewards: 153.4\n",
      "################################################## | Episode  700 | rewards: 127.6\n",
      "################################################## | Episode  800 | rewards: 128.1\n",
      "################################################## | Episode  900 | rewards: 122.4\n",
      "################################################## | Episode 1000 | rewards: 123.2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) n\n"
     ]
    }
   ],
   "source": [
    "while input('Continue Training? ([Y]/n)').upper()!='N':\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps taken: 396, rewards earned: 210.57972816412695\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Save model? ([Y]/n) y\n"
     ]
    }
   ],
   "source": [
    "if input('Save model? ([Y]/n)').upper()=='Y':\n",
    "    agent.policy.save('lunarlander_v2@DQN#keras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This env is relative more difficult and takes time to explore.\n",
    "* Takes time to go through the 500 steps in each episode but this is necessary since the landing only take place at later stage but it awarded the most."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
