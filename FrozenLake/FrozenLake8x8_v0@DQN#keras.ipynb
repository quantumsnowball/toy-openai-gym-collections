{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True).env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Sequential):\n",
    "    def __init__(self, n_embedded=5, n_nodes=32, n_hidden=2):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.add(Embedding(env.observation_space.n, n_embedded, input_length=1))\n",
    "        self.add(Reshape((n_embedded, )))\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden):\n",
    "            self.add(Dense(n_nodes, activation='relu'), )\n",
    "        # output layer\n",
    "        self.add(Dense(env.action_space.n, activation='linear'),)\n",
    "        # compile\n",
    "        self.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    def clone_from(self, another):\n",
    "        self.set_weights(another.get_weights())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self._memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self._memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        samples = random.sample(self._memory, min(len(self._memory), batch_size))\n",
    "        batch = np.array(samples, dtype=object).transpose()\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        states, next_states = np.stack(states), np.stack(next_states)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, policy):\n",
    "        self._env = env\n",
    "        self._memory = Memory(100_000)\n",
    "        self._policy = policy\n",
    "        self._target = DQN().clone_from(self._policy)\n",
    "    \n",
    "    @property\n",
    "    def policy(self): return self._policy\n",
    "    \n",
    "    def choose_action(self, state, *, epilson=0.5):\n",
    "        if np.random.random()>epilson:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(agent._policy(tf.constant([state])))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def play(self, *, n_steps=500, render=False):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        for i_steps in range(1, n_steps+1):\n",
    "            action = self.choose_action(state, epilson=1)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if render: \n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                sleep(0.05)\n",
    "            if done: \n",
    "                break\n",
    "            state = next_state\n",
    "        if render: \n",
    "            print(f'Steps taken: {i_steps}, rewards earned: {rewards}')\n",
    "            env.close()\n",
    "        else:\n",
    "            return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def train(self, *, batch_size=1024, gamma=0.99):\n",
    "        states, actions, rewards, next_states, dones = self._memory.get_batch(batch_size)\n",
    "        next_states_predicts = self._target(next_states).numpy()\n",
    "        q_targets = self._policy(states).numpy()\n",
    "        for i,row in enumerate(q_targets):\n",
    "            row[actions[i]] = rewards[i] if dones[i] else rewards[i] + gamma*np.max(next_states_predicts[i])\n",
    "        X, y = tf.constant(states), tf.constant(q_targets)\n",
    "        self._policy.fit(X, y, epochs=50, batch_size=len(X), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, *, n_eps=1000, n_steps=500):\n",
    "        scores = deque(maxlen=50)\n",
    "        for i_eps in range(1, n_eps+1):            \n",
    "            state = env.reset()\n",
    "            done = False            \n",
    "            for _ in range(n_steps):\n",
    "                action = self.choose_action(state, epilson=i_eps/n_eps)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                self._memory.remember(state, action, reward, next_state, done)\n",
    "                state = next_state \n",
    "                if done: break\n",
    "            self.train()\n",
    "            if i_eps%10==0:\n",
    "                scores.append(self.play())\n",
    "            if i_eps%20==0:\n",
    "                self._target.clone_from(self._policy)\n",
    "            if i_eps%2==0:\n",
    "                print('#', end='')\n",
    "            if i_eps%100==0:                \n",
    "                mean_score = sum(scores)/len(scores)\n",
    "                print(f' | Episode {i_eps:>4d} | rewards: {mean_score:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 0.0\n",
      "################################################## | Episode  200 | rewards: 0.0\n",
      "################################################## | Episode  300 | rewards: 0.0\n",
      "################################################## | Episode  400 | rewards: 0.0\n",
      "################################################## | Episode  500 | rewards: 0.0\n",
      "################################################## | Episode  600 | rewards: 0.0\n",
      "################################################## | Episode  700 | rewards: 0.1\n",
      "################################################## | Episode  800 | rewards: 0.1\n",
      "################################################## | Episode  900 | rewards: 0.1\n",
      "################################################## | Episode 1000 | rewards: 0.2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 0.6\n",
      "################################################## | Episode  200 | rewards: 0.6\n",
      "################################################## | Episode  300 | rewards: 0.6\n",
      "################################################## | Episode  400 | rewards: 0.7\n",
      "################################################## | Episode  500 | rewards: 0.7\n",
      "################################################## | Episode  600 | rewards: 0.6\n",
      "################################################## | Episode  700 | rewards: 0.6\n",
      "################################################## | Episode  800 | rewards: 0.6\n",
      "################################################## | Episode  900 | rewards: 0.5\n",
      "################################################## | Episode 1000 | rewards: 0.5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 0.6\n",
      "################################################## | Episode  200 | rewards: 0.7\n",
      "################################################## | Episode  300 | rewards: 0.7\n",
      "################################################## | Episode  400 | rewards: 0.8\n",
      "################################################## | Episode  500 | rewards: 0.8\n",
      "################################################## | Episode  600 | rewards: 0.8\n",
      "################################################## | Episode  700 | rewards: 0.8\n",
      "################################################## | Episode  800 | rewards: 0.9\n",
      "################################################## | Episode  900 | rewards: 0.8\n",
      "################################################## | Episode 1000 | rewards: 0.8\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue Training? ([Y]/n) n\n"
     ]
    }
   ],
   "source": [
    "while input('Continue Training? ([Y]/n)').upper()=='Y':\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "Steps taken: 66, rewards earned: 1.0\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 8x8 is more difficult because of the larger observation space.\n",
    "* should choose a combination of exploration and exploitation to let the model converge faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
