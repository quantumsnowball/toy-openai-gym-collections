{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True).env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on FrozenLakeEnv in module gym.envs.toy_text.frozen_lake object:\n",
      "\n",
      "class FrozenLakeEnv(gym.envs.toy_text.discrete.DiscreteEnv)\n",
      " |  FrozenLakeEnv(desc=None, map_name='4x4', is_slippery=True)\n",
      " |  \n",
      " |  Winter is here. You and your friends were tossing around a frisbee at the park\n",
      " |  when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      " |  The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      " |  If you step into one of those holes, you'll fall into the freezing water.\n",
      " |  At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      " |  you navigate across the lake and retrieve the disc.\n",
      " |  However, the ice is slippery, so you won't always move in the direction you intend.\n",
      " |  The surface is described using a grid like the following\n",
      " |  \n",
      " |      SFFF\n",
      " |      FHFH\n",
      " |      FFFH\n",
      " |      HFFG\n",
      " |  \n",
      " |  S : starting point, safe\n",
      " |  F : frozen surface, safe\n",
      " |  H : hole, fall to your doom\n",
      " |  G : goal, where the frisbee is located\n",
      " |  \n",
      " |  The episode ends when you reach the goal or fall in a hole.\n",
      " |  You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FrozenLakeEnv\n",
      " |      gym.envs.toy_text.discrete.DiscreteEnv\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, desc=None, map_name='4x4', is_slippery=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  metadata = {'render.modes': ['human', 'ansi']}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.envs.toy_text.discrete.DiscreteEnv:\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the state of the environment and returns an initial observation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, a)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Sequential):\n",
    "    def __init__(self, n_embedded=5, n_nodes=32, n_hidden=2):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.add(Embedding(env.observation_space.n, n_embedded, input_length=1))\n",
    "        self.add(Reshape((n_embedded, )))\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden):\n",
    "            self.add(Dense(n_nodes, activation='relu'), )\n",
    "        # output layer\n",
    "        self.add(Dense(env.action_space.n, activation='linear'),)\n",
    "        # compile\n",
    "        self.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    def clone_from(self, another):\n",
    "        self.set_weights(another.get_weights())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self._memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self._memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        samples = random.sample(self._memory, min(len(self._memory), batch_size))\n",
    "        batch = np.array(samples, dtype=object).transpose()\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        states, next_states = np.stack(states), np.stack(next_states)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, policy):\n",
    "        self._env = env\n",
    "        self._memory = Memory(100_000)\n",
    "        self._policy = policy\n",
    "        self._target = DQN().clone_from(self._policy)\n",
    "    \n",
    "    @property\n",
    "    def policy(self): return self._policy\n",
    "    \n",
    "    def choose_action(self, state, *, epilson=0.5):\n",
    "        if np.random.random()>epilson:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(agent._policy(tf.constant([state])))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def play(self, *, n_steps=500, render=False):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        for i_steps in range(1, n_steps+1):\n",
    "            action = self.choose_action(state, epilson=1)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if render: \n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                sleep(0.05)\n",
    "            if done: \n",
    "                break\n",
    "            state = next_state\n",
    "        if render: \n",
    "            print(f'Steps taken: {i_steps}, rewards earned: {rewards}')\n",
    "            env.close()\n",
    "        else:\n",
    "            return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def train(self, *, batch_size=1024, gamma=0.99):\n",
    "        states, actions, rewards, next_states, dones = self._memory.get_batch(batch_size)\n",
    "        next_states_predicts = self._target(next_states).numpy()\n",
    "        q_targets = self._policy(states).numpy()\n",
    "        for i,row in enumerate(q_targets):\n",
    "            row[actions[i]] = rewards[i] if dones[i] else rewards[i] + gamma*np.max(next_states_predicts[i])\n",
    "        X, y = tf.constant(states), tf.constant(q_targets)\n",
    "        self._policy.fit(X, y, epochs=50, batch_size=len(X), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, *, n_eps=3000, n_steps=500):\n",
    "        scores = deque(maxlen=50)\n",
    "        for i_eps in range(1, n_eps+1):            \n",
    "            state = env.reset()\n",
    "            done = False            \n",
    "            for _ in range(n_steps):\n",
    "                action = self.choose_action(state, epilson=i_eps/n_eps)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                self._memory.remember(state, action, reward, next_state, done)\n",
    "                state = next_state \n",
    "                if done: break\n",
    "            self.train()\n",
    "            if i_eps%10==0:\n",
    "                scores.append(self.play())\n",
    "            if i_eps%20==0:\n",
    "                self._target.clone_from(self._policy)\n",
    "            if i_eps%2==0:\n",
    "                print('#', end='')\n",
    "            if i_eps%100==0:                \n",
    "                mean_score = sum(scores)/len(scores)\n",
    "                print(f' | Episode {i_eps:>4d} | rewards: {mean_score:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: 0.0\n",
      "################################################## | Episode  200 | rewards: 0.1\n",
      "################################################## | Episode  300 | rewards: 0.2\n",
      "################################################## | Episode  400 | rewards: 0.3\n",
      "################################################## | Episode  500 | rewards: 0.4\n",
      "################################################## | Episode  600 | rewards: 0.6\n",
      "################################################## | Episode  700 | rewards: 0.6\n",
      "################################################## | Episode  800 | rewards: 0.7\n",
      "################################################## | Episode  900 | rewards: 0.7\n",
      "################################################## | Episode 1000 | rewards: 0.7\n",
      "################################################## | Episode 1100 | rewards: 0.6\n",
      "################################################## | Episode 1200 | rewards: 0.6\n",
      "################################################## | Episode 1300 | rewards: 0.7\n",
      "################################################## | Episode 1400 | rewards: 0.7\n",
      "################################################## | Episode 1500 | rewards: 0.8\n",
      "################################################## | Episode 1600 | rewards: 0.8\n",
      "################################################## | Episode 1700 | rewards: 0.8\n",
      "################################################## | Episode 1800 | rewards: 0.9\n",
      "################################################## | Episode 1900 | rewards: 0.9\n",
      "################################################## | Episode 2000 | rewards: 0.9\n",
      "################################################## | Episode 2100 | rewards: 0.9\n",
      "################################################## | Episode 2200 | rewards: 0.9\n",
      "################################################## | Episode 2300 | rewards: 0.8\n",
      "################################################## | Episode 2400 | rewards: 0.8\n",
      "################################################## | Episode 2500 | rewards: 0.8\n",
      "################################################## | Episode 2600 | rewards: 0.8\n",
      "################################################## | Episode 2700 | rewards: 0.8\n",
      "################################################## | Episode 2800 | rewards: 0.8\n",
      "################################################## | Episode 2900 | rewards: 0.8\n",
      "################################################## | Episode 3000 | rewards: 0.9\n"
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Steps taken: 35, rewards earned: 1.0\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compared to Q-Learning, DQN can perform much better average reward.\n",
    "* I guess it is because I made use of Replay Memory which gives more change for the network to learn the randomness of slippery.\n",
    "* Also DQN may have access to a richer state space (considering randomness of action produced extra variabilities) and there may be able to learn some randomness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
