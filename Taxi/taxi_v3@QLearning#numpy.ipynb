{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TaxiEnv in module gym.envs.toy_text.taxi object:\n",
      "\n",
      "class TaxiEnv(gym.envs.toy_text.discrete.DiscreteEnv)\n",
      " |  The Taxi Problem\n",
      " |  from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
      " |  by Tom Dietterich\n",
      " |  \n",
      " |  Description:\n",
      " |  There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
      " |  \n",
      " |  Observations: \n",
      " |  There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
      " |  \n",
      " |  Passenger locations:\n",
      " |  - 0: R(ed)\n",
      " |  - 1: G(reen)\n",
      " |  - 2: Y(ellow)\n",
      " |  - 3: B(lue)\n",
      " |  - 4: in taxi\n",
      " |  \n",
      " |  Destinations:\n",
      " |  - 0: R(ed)\n",
      " |  - 1: G(reen)\n",
      " |  - 2: Y(ellow)\n",
      " |  - 3: B(lue)\n",
      " |      \n",
      " |  Actions:\n",
      " |  There are 6 discrete deterministic actions:\n",
      " |  - 0: move south\n",
      " |  - 1: move north\n",
      " |  - 2: move east \n",
      " |  - 3: move west \n",
      " |  - 4: pickup passenger\n",
      " |  - 5: dropoff passenger\n",
      " |  \n",
      " |  Rewards: \n",
      " |  There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
      " |  \n",
      " |  \n",
      " |  Rendering:\n",
      " |  - blue: passenger\n",
      " |  - magenta: destination\n",
      " |  - yellow: empty taxi\n",
      " |  - green: full taxi\n",
      " |  - other letters (R, G, Y and B): locations for passengers and destinations\n",
      " |  \n",
      " |  \n",
      " |  state space is represented by:\n",
      " |      (taxi_row, taxi_col, passenger_location, destination)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TaxiEnv\n",
      " |      gym.envs.toy_text.discrete.DiscreteEnv\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decode(self, i)\n",
      " |  \n",
      " |  encode(self, taxi_row, taxi_col, pass_loc, dest_idx)\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  metadata = {'render.modes': ['human', 'ansi']}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.envs.toy_text.discrete.DiscreteEnv:\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the state of the environment and returns an initial observation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, a)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Steps taken: 3131, penalties incurred: 1065\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "done = False\n",
    "steps, penalties = 0, 0\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if reward==-10:\n",
    "        penalties += 1\n",
    "    steps += 1\n",
    "print(f'Steps taken: {steps}, penalties incurred: {penalties}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000\n",
      "Episode: 2000\n",
      "Episode: 3000\n",
      "Episode: 4000\n",
      "Episode: 5000\n",
      "Episode: 6000\n",
      "Episode: 7000\n",
      "Episode: 8000\n",
      "Episode: 9000\n",
      "Episode: 10000\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1,10001):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.uniform() < epsilon:\n",
    "            # exploration\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # exploitation\n",
    "            action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1-alpha)*old_value + alpha*(reward+gamma*next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        state = next_state\n",
    "    if i%1000==0:\n",
    "        print(f'Episode: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-2.3930052 , -2.33616699, -2.37527098, -2.35314108, -2.27325184,\n",
       "        -9.16157496],\n",
       "       [-1.84909945, -1.49348636, -1.84927848, -1.50895688, -0.7504    ,\n",
       "        -8.90552491],\n",
       "       ...,\n",
       "       [-1.08831672, -0.02591116, -1.08831672, -1.11746961, -2.82266006,\n",
       "        -1.96      ],\n",
       "       [-2.08280307, -2.07093967, -2.08379147, -2.07584331, -2.88086489,\n",
       "        -3.62559153],\n",
       "       [-0.196     , -0.196     ,  0.7058308 , 10.85889867, -1.93574214,\n",
       "        -1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Steps taken: 16, penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "steps, penalties = 0, 0\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if reward==-10:\n",
    "        penalties += 1\n",
    "    steps += 1\n",
    "    state = next_state\n",
    "    sleep(0.1)\n",
    "print(f'Steps taken: {steps}, penalties incurred: {penalties}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this environment has finite state space and finite action space, it could be solved very quick and easily by using a simple Q-table. For any environment having small finite state and action space, Q-Learning may be the most preferred way to try first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
