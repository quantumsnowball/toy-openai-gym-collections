{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Reshape\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TaxiEnv in module gym.envs.toy_text.taxi object:\n",
      "\n",
      "class TaxiEnv(gym.envs.toy_text.discrete.DiscreteEnv)\n",
      " |  The Taxi Problem\n",
      " |  from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
      " |  by Tom Dietterich\n",
      " |  \n",
      " |  Description:\n",
      " |  There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
      " |  \n",
      " |  Observations: \n",
      " |  There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
      " |  \n",
      " |  Passenger locations:\n",
      " |  - 0: R(ed)\n",
      " |  - 1: G(reen)\n",
      " |  - 2: Y(ellow)\n",
      " |  - 3: B(lue)\n",
      " |  - 4: in taxi\n",
      " |  \n",
      " |  Destinations:\n",
      " |  - 0: R(ed)\n",
      " |  - 1: G(reen)\n",
      " |  - 2: Y(ellow)\n",
      " |  - 3: B(lue)\n",
      " |      \n",
      " |  Actions:\n",
      " |  There are 6 discrete deterministic actions:\n",
      " |  - 0: move south\n",
      " |  - 1: move north\n",
      " |  - 2: move east \n",
      " |  - 3: move west \n",
      " |  - 4: pickup passenger\n",
      " |  - 5: dropoff passenger\n",
      " |  \n",
      " |  Rewards: \n",
      " |  There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
      " |  \n",
      " |  \n",
      " |  Rendering:\n",
      " |  - blue: passenger\n",
      " |  - magenta: destination\n",
      " |  - yellow: empty taxi\n",
      " |  - green: full taxi\n",
      " |  - other letters (R, G, Y and B): locations for passengers and destinations\n",
      " |  \n",
      " |  \n",
      " |  state space is represented by:\n",
      " |      (taxi_row, taxi_col, passenger_location, destination)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TaxiEnv\n",
      " |      gym.envs.toy_text.discrete.DiscreteEnv\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decode(self, i)\n",
      " |  \n",
      " |  encode(self, taxi_row, taxi_col, pass_loc, dest_idx)\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  metadata = {'render.modes': ['human', 'ansi']}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.envs.toy_text.discrete.DiscreteEnv:\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the state of the environment and returns an initial observation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, a)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Sequential):\n",
    "    def __init__(self, n_embedded=10, n_nodes=50, n_hidden=2):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.add(Embedding(env.observation_space.n, n_embedded, input_length=1))\n",
    "        self.add(Reshape((n_embedded, )))\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden):\n",
    "            self.add(Dense(n_nodes, activation='relu'), )\n",
    "        # output layer\n",
    "        self.add(Dense(env.action_space.n, activation='linear'),)\n",
    "        # compile\n",
    "        self.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    def clone_from(self, another):\n",
    "        self.set_weights(another.get_weights())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dqn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 8,406\n",
      "Trainable params: 8,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self._memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self._memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        samples = random.sample(self._memory, min(len(self._memory), batch_size))\n",
    "        batch = np.array(samples, dtype=object).transpose()\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        states, next_states = np.stack(states), np.stack(next_states)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize replay memory capacity.\n",
    "* Initialize the policy network with random weights.\n",
    "* Clone the policy network, and call it the target network.\n",
    "* For each episode:\n",
    "    * Initialize the starting state.\n",
    "    * For each time step:\n",
    "        * Select an action.\n",
    "            * Via exploration or exploitation\n",
    "        * Execute selected action in an emulator.\n",
    "        * Observe reward and next state.\n",
    "        * Store experience in replay memory.\n",
    "        * Sample random batch from replay memory.\n",
    "        * Preprocess states from batch.\n",
    "        * Pass batch of preprocessed states to policy network.\n",
    "        * Calculate loss between output Q-values and target Q-values.\n",
    "            * Requires a pass to the target network for the next state\n",
    "        * Gradient descent updates weights in the policy network to minimize loss.\n",
    "            * After time steps, weights in the target network are updated to the weights in the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, policy):\n",
    "        self._env = env\n",
    "        self._memory = Memory(100000)\n",
    "        self._policy = policy\n",
    "        self._target = DQN().clone_from(self._policy)\n",
    "\n",
    "    @property\n",
    "    def policy(self): return self._policy\n",
    "    \n",
    "    def choose_action(self, state, epsilon=1.0):\n",
    "        if np.random.uniform(0,1)<=epsilon:\n",
    "            action = self._env.action_space.sample()            \n",
    "        else:            \n",
    "            action = np.argmax(self._policy(tf.constant([state])))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def play(self, render=False, n_steps=200):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        for i_steps in range(1,n_steps+1):\n",
    "            action = np.argmax(self._policy(tf.constant([state])))\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if render: \n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                sleep(0.05)\n",
    "            if done: \n",
    "                break\n",
    "            state = next_state\n",
    "        if render: \n",
    "            print(f'Steps taken: {steps}, rewards earned: {rewards}')\n",
    "            env.close()\n",
    "        else:\n",
    "            return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):    \n",
    "    def train(self, batch_size=512, gamma=0.99):\n",
    "        states, actions, rewards, next_states, dones = self._memory.get_batch(batch_size)\n",
    "        next_states_predicts = self._target(next_states).numpy()\n",
    "        q_targets = self._policy(states).numpy()        \n",
    "        for i,row in enumerate(q_targets):\n",
    "            row[actions[i]] = rewards[i] if dones[i] else rewards[i] + gamma*np.max(next_states_predicts[i])\n",
    "        X, y = tf.constant(states), tf.constant(q_targets)\n",
    "        self._policy.fit(X, y, epochs=10, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, n_eps=5001, max_steps=200):\n",
    "        scores = deque(maxlen=100)\n",
    "        for i_eps in range(1, n_eps+1):            \n",
    "            state = env.reset()\n",
    "            done = False            \n",
    "            for _ in range(max_steps):\n",
    "                action = self.choose_action(state, epsilon=i_eps/n_eps)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                self._memory.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done: break\n",
    "            self.train()\n",
    "            if i_eps%10==0:\n",
    "                scores.append(self.play(render=False))\n",
    "            if i_eps%50==0:\n",
    "                self._target.clone_from(self._policy)                \n",
    "            if i_eps%2==0:\n",
    "                print('#', end='')\n",
    "                if i_eps%100==0:               \n",
    "                    avg_score = sum(scores)/len(scores)\n",
    "                    print(f' | Episode {i_eps:>4d} | rewards: {avg_score:.1f}', end='\\n')\n",
    "                    if avg_score>12:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: -200.0\n",
      "################################################## | Episode  200 | rewards: -200.0\n",
      "################################################## | Episode  300 | rewards: -200.0\n",
      "################################################## | Episode  400 | rewards: -244.1\n",
      "################################################## | Episode  500 | rewards: -235.3\n",
      "################################################## | Episode  600 | rewards: -259.4\n",
      "################################################## | Episode  700 | rewards: -250.9\n",
      "################################################## | Episode  800 | rewards: -244.6\n",
      "################################################## | Episode  900 | rewards: -239.6\n",
      "################################################## | Episode 1000 | rewards: -235.6\n",
      "################################################## | Episode 1100 | rewards: -235.6\n",
      "################################################## | Episode 1200 | rewards: -235.6\n",
      "################################################## | Episode 1300 | rewards: -235.6\n",
      "################################################## | Episode 1400 | rewards: -218.0\n",
      "################################################## | Episode 1500 | rewards: -218.0\n",
      "################################################## | Episode 1600 | rewards: -200.0\n",
      "################################################## | Episode 1700 | rewards: -200.0\n",
      "################################################## | Episode 1800 | rewards: -200.0\n",
      "################################################## | Episode 1900 | rewards: -195.7\n",
      "################################################## | Episode 2000 | rewards: -193.6\n",
      "################################################## | Episode 2100 | rewards: -191.4\n",
      "################################################## | Episode 2200 | rewards: -191.4\n",
      "################################################## | Episode 2300 | rewards: -191.4\n",
      "################################################## | Episode 2400 | rewards: -191.4\n",
      "################################################## | Episode 2500 | rewards: -187.2\n",
      "################################################## | Episode 2600 | rewards: -176.7\n",
      "################################################## | Episode 2700 | rewards: -168.3\n",
      "################################################## | Episode 2800 | rewards: -149.6\n",
      "################################################## | Episode 2900 | rewards: -135.0\n",
      "################################################## | Episode 3000 | rewards: -116.3\n",
      "################################################## | Episode 3100 | rewards: -97.7\n",
      "################################################## | Episode 3200 | rewards: -76.9\n",
      "################################################## | Episode 3300 | rewards: -56.2\n",
      "################################################## | Episode 3400 | rewards: -35.4\n",
      "################################################## | Episode 3500 | rewards: -18.8\n",
      "################################################## | Episode 3600 | rewards: -8.5\n",
      "################################################## | Episode 3700 | rewards: 3.9\n",
      "################################################## | Episode 3800 | rewards: 5.9\n",
      "################################################## | Episode 3900 | rewards: 7.9\n",
      "################################################## | Episode 4000 | rewards: 7.9\n",
      "################################################## | Episode 4100 | rewards: 7.9\n",
      "################################################## | Episode 4200 | rewards: 8.0\n",
      "################################################## | Episode 4300 | rewards: 8.1\n",
      "################################################## | Episode 4400 | rewards: 8.1\n",
      "################################################## | Episode 4500 | rewards: 8.2\n",
      "################################################## | Episode 4600 | rewards: 8.1\n",
      "################################################## | Episode 4700 | rewards: 8.1\n",
      "################################################## | Episode 4800 | rewards: 8.2\n",
      "################################################## | Episode 4900 | rewards: 8.0\n",
      "################################################## | Episode 5000 | rewards: 8.0\n"
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Steps taken: 11, rewards earned: 10\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Save model? ([Y]/n) y\n"
     ]
    }
   ],
   "source": [
    "if input('Save model? ([Y]/n)').upper()=='Y':\n",
    "    policy.save('taxi_v3_DQN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DQN may be unnecessary for this finite environment, but I tried anyway.\n",
    "* Training time is slower than using a Q-Table, always use the simplest available model.\n",
    "* The replay memory may have increased the number of training samples and therefore increased the training time, but it is necessary to relearn from past experience for more complex environment.\n",
    "* Most solution on the Internet suggests to use two separated networks for policy and target, and only clone the weights of the policy to the target after certain steps to keep the Q-value target stable.\n",
    "* The principal of Q-Learning and DQN are the same, to approximate the `State-Value function` or `Q-function` by exploring the environment and observing the `temporal difference` between two states. If the approximation is good enough, the temporal difference should be minimal and thus satisfying the [`Bellman equation`](https://en.wikipedia.org/wiki/Bellman_equation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
