{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Reshape\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Sequential):\n",
    "    def __init__(self, n_embedded=10, n_nodes=50, n_hidden=2):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.add(Embedding(env.observation_space.n, n_embedded, input_length=1))\n",
    "        self.add(Reshape((n_embedded, )))\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden):\n",
    "            self.add(Dense(n_nodes, activation='relu'), )\n",
    "        # output layer\n",
    "        self.add(Dense(env.action_space.n, activation='linear'),)\n",
    "        # compile\n",
    "        self.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    def clone_from(self, another):\n",
    "        self.set_weights(another.get_weights())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dqn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 8,406\n",
      "Trainable params: 8,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self._memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self._memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        samples = random.sample(self._memory, min(len(self._memory), batch_size))\n",
    "        batch = np.array(samples, dtype=object).transpose()\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        states, next_states = np.stack(states), np.stack(next_states)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize replay memory capacity.\n",
    "* Initialize the policy network with random weights.\n",
    "* Clone the policy network, and call it the target network.\n",
    "* For each episode:\n",
    "    * Initialize the starting state.\n",
    "    * For each time step:\n",
    "        * Select an action.\n",
    "            * Via exploration or exploitation\n",
    "        * Execute selected action in an emulator.\n",
    "        * Observe reward and next state.\n",
    "        * Store experience in replay memory.\n",
    "        * Sample random batch from replay memory.\n",
    "        * Preprocess states from batch.\n",
    "        * Pass batch of preprocessed states to policy network.\n",
    "        * Calculate loss between output Q-values and target Q-values.\n",
    "            * Requires a pass to the target network for the next state\n",
    "        * Gradient descent updates weights in the policy network to minimize loss.\n",
    "            * After time steps, weights in the target network are updated to the weights in the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, policy):\n",
    "        self._env = env\n",
    "        self._memory = Memory(100000)\n",
    "        self._policy = policy\n",
    "        self._target = DQN().clone_from(self._policy)\n",
    "\n",
    "    @property\n",
    "    def policy(self): return self._policy\n",
    "    \n",
    "    def choose_action(self, state, epsilon=1.0):\n",
    "        if np.random.uniform(0,1)<=epsilon:\n",
    "            action = self._env.action_space.sample()            \n",
    "        else:            \n",
    "            action = np.argmax(self._policy(tf.constant([state])))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def play(self, render=False, n_steps=200):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        for i_steps in range(1,n_steps+1):\n",
    "            action = np.argmax(self._policy(tf.constant([state])))\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if render: \n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                sleep(0.05)\n",
    "            if done: \n",
    "                break\n",
    "            state = next_state\n",
    "        if render: \n",
    "            print(f'Steps taken: {steps}, rewards earned: {rewards}')\n",
    "            env.close()\n",
    "        else:\n",
    "            return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):    \n",
    "    def train(self, batch_size=512, gamma=0.99):\n",
    "        states, actions, rewards, next_states, dones = self._memory.get_batch(batch_size)\n",
    "        next_states_predicts = self._target(next_states).numpy()\n",
    "        q_targets = self._policy(states).numpy()        \n",
    "        for i,row in enumerate(q_targets):\n",
    "            row[actions[i]] = rewards[i] if dones[i] else rewards[i] + gamma*np.max(next_states_predicts[i])\n",
    "        X, y = tf.constant(states), tf.constant(q_targets)\n",
    "        self._policy.fit(X, y, epochs=10, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, n_eps=5001, max_steps=200):\n",
    "        scores = deque(maxlen=100)\n",
    "        for i_eps in range(1, n_eps+1):            \n",
    "            state = env.reset()\n",
    "            done = False            \n",
    "            for _ in range(max_steps):\n",
    "                action = self.choose_action(state, epsilon=i_eps/n_eps)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                self._memory.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done: break\n",
    "            self.train()\n",
    "            if i_eps%10==0:\n",
    "                scores.append(self.play(render=False))\n",
    "            if i_eps%50==0:\n",
    "                self._target.clone_from(self._policy)                \n",
    "            if i_eps%2==0:\n",
    "                print('#', end='')\n",
    "                if i_eps%100==0:               \n",
    "                    avg_score = sum(scores)/len(scores)\n",
    "                    print(f' | Episode {i_eps:>4d} | rewards: {avg_score:.1f}', end='\\n')\n",
    "                    if avg_score>12:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## | Episode  100 | rewards: -200.0\n",
      "################################################## | Episode  200 | rewards: -200.0\n",
      "################################################## | Episode  300 | rewards: -200.0\n",
      "################################################## | Episode  400 | rewards: -244.1\n",
      "################################################## | Episode  500 | rewards: -235.3\n",
      "################################################## | Episode  600 | rewards: -259.4\n",
      "################################################## | Episode  700 | rewards: -250.9\n",
      "################################################## | Episode  800 | rewards: -244.6\n",
      "################################################## | Episode  900 | rewards: -239.6\n",
      "################################################## | Episode 1000 | rewards: -235.6\n",
      "################################################## | Episode 1100 | rewards: -235.6\n",
      "################################################## | Episode 1200 | rewards: -235.6\n",
      "################################################## | Episode 1300 | rewards: -235.6\n",
      "################################################## | Episode 1400 | rewards: -218.0\n",
      "################################################## | Episode 1500 | rewards: -218.0\n",
      "################################################## | Episode 1600 | rewards: -200.0\n",
      "################################################## | Episode 1700 | rewards: -200.0\n",
      "################################################## | Episode 1800 | rewards: -200.0\n",
      "################################################## | Episode 1900 | rewards: -195.7\n",
      "################################################## | Episode 2000 | rewards: -193.6\n",
      "################################################## | Episode 2100 | rewards: -191.4\n",
      "################################################## | Episode 2200 | rewards: -191.4\n",
      "################################################## | Episode 2300 | rewards: -191.4\n",
      "################################################## | Episode 2400 | rewards: -191.4\n",
      "################################################## | Episode 2500 | rewards: -187.2\n",
      "################################################## | Episode 2600 | rewards: -176.7\n",
      "################################################## | Episode 2700 | rewards: -168.3\n",
      "################################################## | Episode 2800 | rewards: -149.6\n",
      "################################################## | Episode 2900 | rewards: -135.0\n",
      "################################################## | Episode 3000 | rewards: -116.3\n",
      "################################################## | Episode 3100 | rewards: -97.7\n",
      "################################################## | Episode 3200 | rewards: -76.9\n",
      "################################################## | Episode 3300 | rewards: -56.2\n",
      "################################################## | Episode 3400 | rewards: -35.4\n",
      "################################################## | Episode 3500 | rewards: -18.8\n",
      "################################################## | Episode 3600 | rewards: -8.5\n",
      "################################################## | Episode 3700 | rewards: 3.9\n",
      "################################################## | Episode 3800 | rewards: 5.9\n",
      "################################################## | Episode 3900 | rewards: 7.9\n",
      "################################################## | Episode 4000 | rewards: 7.9\n",
      "################################################## | Episode 4100 | rewards: 7.9\n",
      "################################################## | Episode 4200 | rewards: 8.0\n",
      "################################################## | Episode 4300 | rewards: 8.1\n",
      "################################################## | Episode 4400 | rewards: 8.1\n",
      "################################################## | Episode 4500 | rewards: 8.2\n",
      "################################################## | Episode 4600 | rewards: 8.1\n",
      "################################################## | Episode 4700 | rewards: 8.1\n",
      "################################################## | Episode 4800 | rewards: 8.2\n",
      "################################################## | Episode 4900 | rewards: 8.0\n",
      "################################################## | Episode 5000 | rewards: 8.0\n"
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Steps taken: 11, rewards earned: 10\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Save model? ([Y]/n) y\n"
     ]
    }
   ],
   "source": [
    "if input('Save model? ([Y]/n)').upper()=='Y':\n",
    "    policy.save('taxi_v3_DQN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DQN may be unnecessary for this finite environment, but I tried anyway.\n",
    "* Training time is slower than using a Q-Table, always use the simplest available model.\n",
    "* The replay memory may have increased the number of training samples and therefore increased the training time, but it is necessary to relearn from past experience for more complex environment.\n",
    "* Most solution on the Internet suggests to use two separated networks for policy and target, and only clone the weights of the policy to the target after certain steps to keep the Q-value target stable.\n",
    "* The principal of Q-Learning and DQN are the same, to approximate the `State-Value function` or `Q-function` by exploring the environment and observing the `temporal difference` between two states. If the approximation is good enough, the temporal difference should be minimal and thus satisfying the [`Bellman equation`](https://en.wikipedia.org/wiki/Bellman_equation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
