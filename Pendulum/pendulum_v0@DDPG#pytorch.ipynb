{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.logger.set_level(40) # subpress warnings\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-8.0, 8.0, (3,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1., -1., -8.], dtype=float32), array([1., 1., 8.], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low, env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.], dtype=float32), array([2.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.low, env.action_space.high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, *, n_hidden=128, lr=.0025):\n",
    "        super().__init__()\n",
    "        # layers\n",
    "        self._in_s_0 = nn.Linear(state_shape[0], n_hidden)\n",
    "        self._hd_s_1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self._in_a_0 = nn.Linear(action_shape[0], n_hidden)\n",
    "        self._hd_a_1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self._q = nn.Linear(n_hidden, action_shape[0])\n",
    "        # optimizer\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        # state value\n",
    "        sv = T.relu(self._in_s_0(state))\n",
    "        sv = self._hd_s_1(sv)\n",
    "        # action value\n",
    "        av = T.relu(self._in_a_0(action))\n",
    "        av = self._hd_a_1(av)\n",
    "        # state-action value\n",
    "        sav = T.relu(T.add(sv, av))\n",
    "        sav = self._q(sav)\n",
    "        return sav        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, *, n_hidden=128, lr=.0025):\n",
    "        super().__init__()\n",
    "        # layers\n",
    "        self._in_0 = nn.Linear(state_shape[0], n_hidden)\n",
    "        self._hd_1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self._mu = nn.Linear(n_hidden, action_shape[0])\n",
    "        # optimizer\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        s = T.relu(self._in_0(state))\n",
    "        s = T.relu(self._hd_1(s))\n",
    "        mu = self._mu(s)\n",
    "        return mu       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self._memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def __len__(self): return len(self._memory)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self._memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_batch(self, size):\n",
    "        # sampling\n",
    "        samples = random.sample(self._memory, min(len(self), size))\n",
    "        # reshape\n",
    "        states, actions, rewards, next_states, dones = np.array(samples).transpose()\n",
    "        states, next_states = np.stack(states), np.stack(next_states)\n",
    "        actions, rewards, dones = (a.reshape((states.shape[0],-1)) \n",
    "            for a in (actions, rewards, dones))\n",
    "        # convert to tensors\n",
    "        states = T.as_tensor(states.astype(float), dtype=T.float32)\n",
    "        actions = T.as_tensor(actions.astype(float), dtype=T.float32)\n",
    "        rewards = T.as_tensor(rewards.astype(float), dtype=T.float32)\n",
    "        next_states = T.as_tensor(next_states.astype(float), dtype=T.float32)\n",
    "        dones = T.as_tensor(dones.astype(int), dtype=T.int32)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        self._memory = Memory(100000)\n",
    "        # create networks\n",
    "        state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "        self._actor = ActorNet(state_shape, action_shape)\n",
    "        self._actor_target = ActorNet(state_shape, action_shape)\n",
    "        self._critic = CriticNet(state_shape, action_shape)\n",
    "        self._critic_target = CriticNet(state_shape, action_shape) \n",
    "        # clone weights to target\n",
    "        self.update_targets(0.0)\n",
    "    \n",
    "    def choose_action(self, state, i_step=None, n_step=None, *, noise=True):\n",
    "        with T.no_grad():\n",
    "            state = T.tensor(state, dtype=T.float32)\n",
    "            mu = self._actor(state).detach().numpy()\n",
    "            if noise:\n",
    "                sd = max(0.02, (1-i_step/n_step)*self._env.action_space.high)\n",
    "                noise = np.random.normal(0, sd, mu.shape)\n",
    "                return mu + noise\n",
    "            else:\n",
    "                return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def play(self, *, render=False, summary=True):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        for i_step in count():\n",
    "            action = self.choose_action(state, i_step, noise=False)\n",
    "            next_state, reward, done, info = env.step(action)            \n",
    "            rewards += reward\n",
    "            state = next_state\n",
    "            if render: \n",
    "                env.render()\n",
    "            if done: \n",
    "                break\n",
    "        if render:\n",
    "            if summary:\n",
    "                print(f'Steps taken: {i_step}, rewards earned: {rewards:.4f}')\n",
    "            env.close()\n",
    "        else:\n",
    "            return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def learn(self, batch_size, gamma=0.99):\n",
    "        if len(self._memory) < batch_size: return\n",
    "        # get batch\n",
    "        states, actions, rewards, next_states, dones = self._memory.get_batch(batch_size)\n",
    "        # calculate targets\n",
    "        a_targ = self._actor_target(next_states)\n",
    "        c_vals = self._critic(states, actions)\n",
    "        c_vals_targ = rewards + gamma * (1-dones) * self._critic_target(next_states, a_targ)\n",
    "        # critic loss\n",
    "        self._critic.optim.zero_grad()\n",
    "        critic_loss = F.mse_loss(c_vals_targ, c_vals)\n",
    "        critic_loss.backward()\n",
    "        self._critic.optim.step()\n",
    "        # actor loss\n",
    "        self._actor.optim.zero_grad()\n",
    "        mu = self._actor(states)\n",
    "        actor_loss = T.mean(-self._critic(states, mu))\n",
    "        actor_loss.backward()\n",
    "        self._actor.optim.step()\n",
    "        # update targets\n",
    "        self.update_targets()\n",
    "        \n",
    "    def update_targets(self, polyak=0.99):\n",
    "        for m, mT in ((self._actor, self._actor_target), (self._critic, self._critic_target)):\n",
    "            dd, ddT, new_ddT = dict(m.named_parameters()), dict(mT.named_parameters()), dict()\n",
    "            for name in ddT.keys():\n",
    "                new_ddT[name] = polyak * ddT[name] + (1-polyak) * dd[name]\n",
    "            mT.load_state_dict(new_ddT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, n_eps=200, batch_size=64):\n",
    "        score_hist = deque(maxlen=10)\n",
    "        for i_eps in range(1, n_eps+1):\n",
    "            score = 0\n",
    "            state = self._env.reset()\n",
    "            done = False\n",
    "            for i_step in count():\n",
    "                action = self.choose_action(state, i_step, self._env._max_episode_steps)\n",
    "                next_state, reward, done, info = self._env.step(action)\n",
    "                self._memory.remember(state, action, reward, next_state, done)\n",
    "                self.learn(batch_size)\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                if done: \n",
    "                    break\n",
    "            score_hist.append(score)\n",
    "            if i_eps%1==0:\n",
    "                print('#', end='')\n",
    "            if i_eps%1==0:\n",
    "                score_mean = np.mean(score_hist)\n",
    "                print(f' | Episode {i_eps:>4d} | score: {score:+7.1f} | rolling mean: {score_mean:+7.1f}')\n",
    "                # self.play(render=True, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# | Episode    1 | score: -1379.0 | rolling mean: -1379.0\n",
      "# | Episode    2 | score: -1572.9 | rolling mean: -1475.9\n",
      "# | Episode    3 | score: -1461.1 | rolling mean: -1471.0\n",
      "# | Episode    4 | score: -1383.2 | rolling mean: -1449.0\n",
      "# | Episode    5 | score: -1470.4 | rolling mean: -1453.3\n",
      "# | Episode    6 | score: -1265.1 | rolling mean: -1422.0\n",
      "# | Episode    7 | score: -1357.7 | rolling mean: -1412.8\n",
      "# | Episode    8 | score: -1500.7 | rolling mean: -1423.8\n",
      "# | Episode    9 | score: -1512.8 | rolling mean: -1433.7\n",
      "# | Episode   10 | score: -1392.6 | rolling mean: -1429.6\n",
      "# | Episode   11 | score: -1367.4 | rolling mean: -1428.4\n",
      "# | Episode   12 | score: -1511.4 | rolling mean: -1422.2\n",
      "# | Episode   13 | score: -1365.9 | rolling mean: -1412.7\n",
      "# | Episode   14 | score:  -870.1 | rolling mean: -1361.4\n",
      "# | Episode   15 | score: -1418.9 | rolling mean: -1356.3\n",
      "# | Episode   16 | score: -1525.4 | rolling mean: -1382.3\n",
      "# | Episode   17 | score:  -963.7 | rolling mean: -1342.9\n",
      "# | Episode   18 | score:  -668.8 | rolling mean: -1259.7\n",
      "# | Episode   19 | score:  -871.3 | rolling mean: -1195.5\n",
      "# | Episode   20 | score:  -871.7 | rolling mean: -1143.4\n",
      "# | Episode   21 | score:  -960.7 | rolling mean: -1102.8\n",
      "# | Episode   22 | score:  -880.7 | rolling mean: -1039.7\n",
      "# | Episode   23 | score:  -264.1 | rolling mean:  -929.5\n",
      "# | Episode   24 | score:  -392.1 | rolling mean:  -881.7\n",
      "# | Episode   25 | score:  -392.2 | rolling mean:  -779.0\n",
      "# | Episode   26 | score:  -395.6 | rolling mean:  -666.1\n",
      "# | Episode   27 | score:  -257.5 | rolling mean:  -595.5\n",
      "# | Episode   28 | score:  -261.0 | rolling mean:  -554.7\n",
      "# | Episode   29 | score:  -267.3 | rolling mean:  -494.3\n",
      "# | Episode   30 | score:  -260.6 | rolling mean:  -433.2\n",
      "# | Episode   31 | score:  -129.3 | rolling mean:  -350.0\n",
      "# | Episode   32 | score:  -138.3 | rolling mean:  -275.8\n",
      "# | Episode   33 | score:  -134.7 | rolling mean:  -262.8\n",
      "# | Episode   34 | score:  -140.8 | rolling mean:  -237.7\n",
      "# | Episode   35 | score:  -372.0 | rolling mean:  -235.7\n",
      "# | Episode   36 | score:  -251.6 | rolling mean:  -221.3\n",
      "# | Episode   37 | score:  -133.7 | rolling mean:  -208.9\n",
      "# | Episode   38 | score:  -135.0 | rolling mean:  -196.3\n",
      "# | Episode   39 | score:  -277.9 | rolling mean:  -197.4\n",
      "# | Episode   40 | score:  -139.6 | rolling mean:  -185.3\n",
      "# | Episode   41 | score:  -383.8 | rolling mean:  -210.8\n",
      "# | Episode   42 | score:  -250.4 | rolling mean:  -222.0\n",
      "# | Episode   43 | score:  -136.5 | rolling mean:  -222.2\n",
      "# | Episode   44 | score:  -262.1 | rolling mean:  -234.3\n",
      "# | Episode   45 | score:  -248.8 | rolling mean:  -222.0\n",
      "# | Episode   46 | score:  -133.3 | rolling mean:  -210.1\n",
      "# | Episode   47 | score:  -233.0 | rolling mean:  -220.1\n",
      "# | Episode   48 | score:  -352.1 | rolling mean:  -241.8\n",
      "# | Episode   49 | score:  -138.5 | rolling mean:  -227.8\n",
      "# | Episode   50 | score:  -237.5 | rolling mean:  -237.6\n"
     ]
    }
   ],
   "source": [
    "agent.run(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps taken: 199, rewards earned: -125.8101\n"
     ]
    }
   ],
   "source": [
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* off-policy model for continuous action space.\n",
    "* choose of activation function (i.e. relu vs tanh vs linear) is crucial. For example, must not use relu at any output layers (including intermediate state value output layer before T.add) because that will trim all negative value information and fail the model.\n",
    "* never put unused layers in nn.Module constructor, it mess up the trainable parameters pool and failed the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
